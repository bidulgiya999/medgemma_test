{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOixhMtjVscXwHj+H+p/uAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bidulgiya999/medgemma_test/blob/main/csv%ED%8C%8C%EC%9D%BC_%EC%B6%94%EC%B6%9C%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 이게 신민재씨가 작성한 프롬프"
      ],
      "metadata": {
        "id": "gEp94GRCcuRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C:\\Users\\user\\Desktop\\dataset ├─ img\\01,02,03\\... (이미지들) └─ label\\01,02,03\\... (각 이미지에 대응하는 json) 위와 같은 구조의 디렉토리에서 다음을 작업을 수행하려고 해. 1. label\\01, label\\02, label\\03 디렉토리 안에 각각 약 1100개의 디랙토리가 존재함. 2. 약 1100개의 디랙토리 안에 여러 json파일이 존재함. 3. 0001_01_F_00.json, 0001_01_F_01.json, 0001_01_F_02.json, ... ,0001_01_F_08.json 의 파일에서만 데이터를 추출할 것. 4. 0001_01_F_00.json 에는 {\"info\": {\"filename\": \"0001_01_F.jpg\", \"id\": \"0001\", \"gender\": \"F\", \"age\": 55, \"date\": \"2023-07-27\", \"skin_type\": 3, \"sensitive\": 0}, \"images\": {\"device\": 0, \"width\": 2136, \"height\": 3216, \"angle\": 0, \"facepart\": 0, \"bbox\": [0, 0, 2136, 3216]}, \"annotations\": {\"acne\": null}, \"equipment\": {\"pigmentation_count\": 147}} 내용이 있으며, \"skin_type\": 3, \"sensitive\": 0만을 추출하여 CSV파일에 저장할 것. 5. 나머지 파일에 대해서는 {\"info\": {\"filename\": \"0001_01_F.jpg\", \"id\": \"0001\", \"gender\": \"F\", \"age\": 55, \"date\": \"2023-07-27\", \"skin_type\": 3, \"sensitive\": 0}, \"images\": {\"device\": 0, \"width\": 2136, \"height\": 3216, \"angle\": 0, \"facepart\": 1, \"bbox\": [469, 661, 1638, 1197]}, \"annotations\": {\"forehead_pigmentation\": 1, \"forehead_wrinkle\": 3}, \"equipment\": {\"forehead_moisture\": 53.0, \"forehead_elasticity_R0\": 0.167, \"forehead_elasticity_R1\": 0.058, \"forehead_elasticity_R2\": 0.653, \"forehead_elasticity_R3\": 0.208, \"forehead_elasticity_R4\": 0.085, \"forehead_elasticity_R5\": 0.765, \"forehead_elasticity_R6\": 0.965, \"forehead_elasticity_R7\": 0.389, \"forehead_elasticity_R8\": 0.109, \"forehead_elasticity_R9\": 0.041, \"forehead_elasticity_Q0\": 33.4, \"forehead_elasticity_Q1\": 0.589, \"forehead_elasticity_Q2\": 0.478, \"forehead_elasticity_Q3\": 0.111}} 와 같음. 이 중 \"annotations\": {\"forehead_pigmentation\": 1, \"forehead_wrinkle\": 3}값만을 추출하여 CSV 파일에 저장할 것. 6. 1100개의 디렉토리안에 JSON파일을 모두 순회하면 label\\02, label\\03 순으로 모두 시행하고, CSV파일은 01, 02, 03 총 3개 작성할 것."
      ],
      "metadata": {
        "id": "05iEvwjmclwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fn4ckrib1s4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "# ====== 설정 ======\n",
        "# Windows 경로 예시: C:\\Users\\user\\Desktop\\dataset\n",
        "DATASET_ROOT = Path(r\"C:\\Users\\user\\Desktop\\dataset\")\n",
        "\n",
        "# label 하위의 01, 02, 03 각각을 처리하여 CSV 3개 생성\n",
        "LABEL_ROOT = DATASET_ROOT / \"label\"\n",
        "LABEL_GROUPS = [\"01\", \"02\", \"03\"]  # 처리할 그룹\n",
        "\n",
        "\n",
        "def safe_get(d: Dict[str, Any], path: str, default=None):\n",
        "    \"\"\"\n",
        "    dict에서 'a.b.c' 형태로 안전하게 값을 조회.\n",
        "    키가 없거나 타입이 dict가 아니면 default 반환.\n",
        "    \"\"\"\n",
        "    cur = d\n",
        "    for key in path.split(\".\"):\n",
        "        if not isinstance(cur, dict) or key not in cur:\n",
        "            return default\n",
        "        cur = cur[key]\n",
        "    return cur\n",
        "\n",
        "\n",
        "def want_this_file(name: str) -> bool:\n",
        "    \"\"\"\n",
        "    파일명이 *_00.json ~ *_08.json 인지 여부만 True.\n",
        "    예: 0001_01_F_00.json, 0001_01_F_01.json, ... , 0001_01_F_08.json\n",
        "    \"\"\"\n",
        "    if not name.lower().endswith(\".json\"):\n",
        "        return False\n",
        "    stem = name[:-5]  # .json 제거\n",
        "    # 뒤에서 언더스코어 이후 두 자리 숫자를 추출\n",
        "    parts = stem.rsplit(\"_\", 1)\n",
        "    if len(parts) != 2:\n",
        "        return False\n",
        "    suffix = parts[1]\n",
        "    if len(suffix) != 2 or not suffix.isdigit():\n",
        "        return False\n",
        "    n = int(suffix)\n",
        "    return 0 <= n <= 8\n",
        "\n",
        "\n",
        "def extract_row(json_data: Dict[str, Any], label_group: str, subdir: Path, file_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    규칙에 따라 필요한 값만 추출해 행(dict)으로 반환.\n",
        "    - *_00.json: info.skin_type, info.sensitive\n",
        "    - 나머지 *_01~*_08.json: annotations.forehead_pigmentation, annotations.forehead_wrinkle\n",
        "    공통 메타: label_group, subdir_name, filename(json), image_id(info.id), gender(info.gender), age(info.age), image_filename(info.filename)\n",
        "    \"\"\"\n",
        "    # 공통 메타\n",
        "    info_id = safe_get(json_data, \"info.id\")\n",
        "    gender = safe_get(json_data, \"info.gender\")\n",
        "    age = safe_get(json_data, \"info.age\")\n",
        "    image_filename = safe_get(json_data, \"info.filename\")\n",
        "\n",
        "    # 접미사 번호\n",
        "    suffix = file_path.stem.rsplit(\"_\", 1)[-1]  # \"00\"..\"08\"\n",
        "    is_00 = (suffix == \"00\")\n",
        "\n",
        "    row = {\n",
        "        \"label_group\": label_group,                 # \"01\"/\"02\"/\"03\"\n",
        "        \"subdir_name\": subdir.name,                 # 하위 1100개 디렉토리명\n",
        "        \"json_file\": file_path.name,                # JSON 파일명\n",
        "        \"json_suffix\": suffix,                      # \"00\"..\"08\"\n",
        "        \"image_id\": info_id,\n",
        "        \"gender\": gender,\n",
        "        \"age\": age,\n",
        "        \"image_filename\": image_filename,\n",
        "        # 아래 필드는 상황에 따라 채움 (없으면 빈칸)\n",
        "        \"skin_type\": \"\",\n",
        "        \"sensitive\": \"\",\n",
        "        \"forehead_pigmentation\": \"\",\n",
        "        \"forehead_wrinkle\": \"\",\n",
        "    }\n",
        "\n",
        "    if is_00:\n",
        "        row[\"skin_type\"] = safe_get(json_data, \"info.skin_type\", \"\")\n",
        "        row[\"sensitive\"] = safe_get(json_data, \"info.sensitive\", \"\")\n",
        "    else:\n",
        "        row[\"forehead_pigmentation\"] = safe_get(json_data, \"annotations.forehead_pigmentation\", \"\")\n",
        "        row[\"forehead_wrinkle\"] = safe_get(json_data, \"annotations.forehead_wrinkle\", \"\")\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "def process_one_label_group(label_group: str, label_root: Path) -> Path:\n",
        "    \"\"\"\n",
        "    label/<label_group> 아래의 약 1100개 하위 디렉토리를 순회하며\n",
        "    *_00.json~*_08.json 파일만 처리해서 CSV로 저장.\n",
        "    반환: 생성된 CSV 경로\n",
        "    \"\"\"\n",
        "    group_dir = label_root / label_group\n",
        "    if not group_dir.exists():\n",
        "        raise FileNotFoundError(f\"[ERROR] 디렉토리가 존재하지 않습니다: {group_dir}\")\n",
        "\n",
        "    out_csv = group_dir / f\"{label_group}.csv\"\n",
        "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # CSV 헤더 정의\n",
        "    fieldnames = [\n",
        "        \"label_group\", \"subdir_name\", \"json_file\", \"json_suffix\",\n",
        "        \"image_id\", \"gender\", \"age\", \"image_filename\",\n",
        "        \"skin_type\", \"sensitive\",\n",
        "        \"forehead_pigmentation\", \"forehead_wrinkle\",\n",
        "    ]\n",
        "\n",
        "    count_files = 0\n",
        "    count_rows = 0\n",
        "\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        # 하위 디렉토리(약 1100개) 순회\n",
        "        for subdir in sorted([p for p in group_dir.iterdir() if p.is_dir()]):\n",
        "            # 각 하위 디렉토리 안의 JSON 파일들 중 원하는 패턴만 처리\n",
        "            json_files = [p for p in subdir.glob(\"*.json\") if want_this_file(p.name)]\n",
        "            for jp in sorted(json_files):\n",
        "                count_files += 1\n",
        "                try:\n",
        "                    with open(jp, \"r\", encoding=\"utf-8\") as jf:\n",
        "                        data = json.load(jf)\n",
        "                    row = extract_row(data, label_group, subdir, jp)\n",
        "                    writer.writerow(row)\n",
        "                    count_rows += 1\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"[WARN] JSON 파싱 실패: {jp}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[WARN] 처리 중 예외 발생: {jp} -> {e}\")\n",
        "\n",
        "    print(f\"[INFO] 그룹 {label_group}: 처리 파일 {count_files}개, 기록 행 {count_rows}개 → {out_csv}\")\n",
        "    return out_csv\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(f\"[INFO] 데이터 루트: {DATASET_ROOT}\")\n",
        "    print(f\"[INFO] 라벨 루트:  {LABEL_ROOT}\")\n",
        "\n",
        "    for g in LABEL_GROUPS:\n",
        "        try:\n",
        "            process_one_label_group(g, LABEL_ROOT)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] 그룹 {g} 처리 실패: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 아래가 내 프롬프트"
      ],
      "metadata": {
        "id": "x-a1vB_6dKY3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  File \"/tmp/ipython-input-128936180.py\", line 2\n",
        "    현재 생성된 파일을 보면 label_group,subdir_name,json_file,json_suffix,image_id,gender,age,image_filename,skin_type,sensitive,forehead_pigmentation,forehead_wrinkle 01,0001,0001_01_F_00.json,00,0001,F,55,0001_01_F.jpg,3,0,, 01,0001,0001_01_F_01.json,01,0001,F,55,0001_01_F.jpg,,,1,3 01,0001,0001_01_F_02.json,02,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_03.json,03,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_04.json,04,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_05.json,05,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_06.json,06,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_07.json,07,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_08.json,08,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_Fb_00.json,00,0001,F,55,0001_01_Fb.jpg,3,0,, 01,0001,0001_01_Fb_01.json,01,0001,F,55,0001_01_Fb.jpg,,,1,3 01,0001,0001_01_Fb_02.json,02,0001,F,55,0001_01_Fb.jpg,,,, 01,0001,0001_01_Fb_03.json,03,0001,F,55,0001_01_Fb.jpg,,,, 01,0001,0001_01_Fb_04.json,04,0001,F,55,0001_01_Fb.jpg,,,, 01,0001,0001_01_Fb_05.json,05,0001,F,55,0001_01_Fb.jpg,,,, 01,0001,0001_01_Fb_06.json,06,0001,F,55,0001_01_Fb.jpg,,,, 01,0001,0001_01_Fb_07.json,07,0001,F,55,0001_01_Fb.jpg,,,, 01,0001,0001_01_Fb_08.json,08,0001,F,55,0001_01_Fb.jpg,,,, 01,0001,0001_01_Ft_00.json,00,0001,F,55,0001_01_Ft.jpg,3,0,, 01,0001,0001_01_Ft_01.json,01,0001,F,55,0001_01_Ft.jpg,,,1,3 01,0001,0001_01_Ft_02.json,02,0001,F,55,0001_01_Ft.jpg,,,, 01,0001,0001_01_Ft_03.json,03,0001,F,55,0001_01_Ft.jpg,,,, 01,0001,0001_01_Ft_04.json,04,0001,F,55,...\n",
        "       이렇게 나오는데 정보손실이 있는거 같아                                                                                                                                                         \n"
      ],
      "metadata": {
        "id": "2WsB7xWNdBYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "# ====== 설정 ======\n",
        "DATASET_ROOT = Path(r\"C:\\Users\\user\\Desktop\\dataset\")\n",
        "LABEL_ROOT = DATASET_ROOT / \"label\"\n",
        "LABEL_GROUPS = [\"01\", \"02\", \"03\"]\n",
        "\n",
        "def safe_get(d, path, default=None):\n",
        "    cur = d\n",
        "    for k in path.split(\".\"):\n",
        "        if not isinstance(cur, dict) or k not in cur:\n",
        "            return default\n",
        "        cur = cur[k]\n",
        "    return cur\n",
        "\n",
        "def parse_name_and_suffix(json_path: Path):\n",
        "    \"\"\"\n",
        "    0001_01_F_00.json -> base='0001_01_F', suffix='00'\n",
        "    \"\"\"\n",
        "    stem = json_path.stem  # without .json\n",
        "    parts = stem.rsplit(\"_\", 1)\n",
        "    if len(parts) != 2 or not parts[1].isdigit() or len(parts[1]) != 2:\n",
        "        return None, None\n",
        "    return parts[0], parts[1]\n",
        "\n",
        "def want_this_file(name: str) -> bool:\n",
        "    if not name.lower().endswith(\".json\"):\n",
        "        return False\n",
        "    stem = name[:-5]\n",
        "    parts = stem.rsplit(\"_\", 1)\n",
        "    if len(parts) != 2:\n",
        "        return False\n",
        "    suf = parts[1]\n",
        "    return suf.isdigit() and len(suf) == 2 and 0 <= int(suf) <= 8\n",
        "\n",
        "def process_group(label_group: str):\n",
        "    group_dir = LABEL_ROOT / label_group\n",
        "    if not group_dir.exists():\n",
        "        print(f\"[WARN] group not found: {group_dir}\")\n",
        "        return\n",
        "\n",
        "    # 누적 딕셔너리: key=(subdir_name, base_name) → 한 행(와이드)\n",
        "    rows = {}\n",
        "\n",
        "    for subdir in sorted([p for p in group_dir.iterdir() if p.is_dir()]):\n",
        "        for jp in sorted([p for p in subdir.glob(\"*.json\") if want_this_file(p.name)]):\n",
        "            base, suf = parse_name_and_suffix(jp)\n",
        "            if base is None:\n",
        "                continue\n",
        "            key = (subdir.name, base)\n",
        "\n",
        "            # 최초 행 생성\n",
        "            if key not in rows:\n",
        "                rows[key] = {\n",
        "                    \"label_group\": label_group,\n",
        "                    \"subdir_name\": subdir.name,\n",
        "                    \"base_name\": base,          # 예: 0001_01_F\n",
        "                    \"image_id\": \"\",\n",
        "                    \"gender\": \"\",\n",
        "                    \"age\": \"\",\n",
        "                    \"image_filename\": \"\",\n",
        "                    \"skin_type\": \"\",\n",
        "                    \"sensitive\": \"\",\n",
        "                }\n",
        "                # suffix별 컬럼 초기화\n",
        "                for i in range(1, 9):  # 01..08\n",
        "                    s = f\"{i:02d}\"\n",
        "                    rows[key][f\"forehead_pigmentation_{s}\"] = \"\"\n",
        "                    rows[key][f\"forehead_wrinkle_{s}\"] = \"\"\n",
        "\n",
        "            try:\n",
        "                with open(jp, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data = json.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] JSON read error: {jp} -> {e}\")\n",
        "                continue\n",
        "\n",
        "            # 공통 메타(가능하면 한 번만 세팅)\n",
        "            if not rows[key][\"image_id\"]:\n",
        "                rows[key][\"image_id\"] = safe_get(data, \"info.id\", \"\")\n",
        "            if not rows[key][\"gender\"]:\n",
        "                rows[key][\"gender\"] = safe_get(data, \"info.gender\", \"\")\n",
        "            if not rows[key][\"age\"]:\n",
        "                rows[key][\"age\"] = safe_get(data, \"info.age\", \"\")\n",
        "            if not rows[key][\"image_filename\"]:\n",
        "                rows[key][\"image_filename\"] = safe_get(data, \"info.filename\", \"\")\n",
        "\n",
        "            if suf == \"00\":\n",
        "                # _00 → skin_type / sensitive\n",
        "                rows[key][\"skin_type\"] = safe_get(data, \"info.skin_type\", rows[key][\"skin_type\"])\n",
        "                rows[key][\"sensitive\"] = safe_get(data, \"info.sensitive\", rows[key][\"sensitive\"])\n",
        "            else:\n",
        "                # _01.._08 → annotations.forehead_pigmentation / forehead_wrinkle\n",
        "                fp = safe_get(data, \"annotations.forehead_pigmentation\", \"\")\n",
        "                fw = safe_get(data, \"annotations.forehead_wrinkle\", \"\")\n",
        "                # 일부 파일은 키가 없을 수 있으므로 덮어쓰기 조건부\n",
        "                if fp != \"\":\n",
        "                    rows[key][f\"forehead_pigmentation_{suf}\"] = fp\n",
        "                if fw != \"\":\n",
        "                    rows[key][f\"forehead_wrinkle_{suf}\"] = fw\n",
        "\n",
        "    # CSV 헤더 구성\n",
        "    fieldnames = [\n",
        "        \"label_group\", \"subdir_name\", \"base_name\",\n",
        "        \"image_id\", \"gender\", \"age\", \"image_filename\",\n",
        "        \"skin_type\", \"sensitive\",\n",
        "    ] + [f\"forehead_pigmentation_{i:02d}\" for i in range(1, 9)] \\\n",
        "      + [f\"forehead_wrinkle_{i:02d}\" for i in range(1, 9)]\n",
        "\n",
        "    out_csv = (group_dir / f\"{label_group}_wide.csv\")\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        w.writeheader()\n",
        "        for key in sorted(rows.keys()):\n",
        "            w.writerow(rows[key])\n",
        "\n",
        "    print(f\"[INFO] 그룹 {label_group}: {len(rows)}행 출력 → {out_csv}\")\n",
        "\n",
        "def main():\n",
        "    for g in LABEL_GROUPS:\n",
        "        process_group(g)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ErqnVVy_dMp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "label_group,subdir_name,json_file,json_suffix,image_id,gender,age,image_filename,skin_type,sensitive,forehead_pigmentation,forehead_wrinkle 여기 나와있는데로 정보를 얻고싶어 01,0001,0001_01_F_00.json,00,0001,F,55,0001_01_F.jpg,3,0,, 01,0001,0001_01_F_01.json,01,0001,F,55,0001_01_F.jpg,,,1,3 01,0001,0001_01_F_02.json,02,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_03.json,03,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_04.json,04,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_05.json,05,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_06.json,06,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_07.json,07,0001,F,55,0001_01_F.jpg,,,, 01,0001,0001_01_F_08.json,08,0001,F,55,0001_01_F.jpg,,,, 근데 이런식으로 생략된것도 있는거 같아"
      ],
      "metadata": {
        "id": "zOjixCMEdOO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def iter_json_files(subdir: Path):\n",
        "    # *.json, *.JSON 모두 수집\n",
        "    for p in subdir.glob(\"*.json\"):\n",
        "        yield p\n",
        "    for p in subdir.glob(\"*.JSON\"):\n",
        "        yield p\n",
        "\n",
        "def want_this_file(name: str) -> bool:\n",
        "    # *_00.json ~ *_08.json (확장자 대소문자 무관)\n",
        "    lower = name.lower()\n",
        "    if not lower.endswith(\".json\"):\n",
        "        return False\n",
        "    stem = lower[:-5]\n",
        "    parts = stem.rsplit(\"_\", 1)\n",
        "    if len(parts) != 2:\n",
        "        return False\n",
        "    suf = parts[1]\n",
        "    return suf.isdigit() and len(suf) == 2 and 0 <= int(suf) <= 8\n"
      ],
      "metadata": {
        "id": "I0uEPvG-c12r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_files = [p for p in iter_json_files(subdir) if want_this_file(p.name)]\n"
      ],
      "metadata": {
        "id": "iEpHlBcfdYjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, csv\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET_ROOT = Path(r\"C:\\Users\\user\\Desktop\\dataset\")\n",
        "LABEL_ROOT = DATASET_ROOT / \"label\"\n",
        "LABEL_GROUPS = [\"01\", \"02\", \"03\"]\n",
        "\n",
        "def safe_get(d, path, default=None, _exist_flag=False):\n",
        "    cur = d\n",
        "    for k in path.split(\".\"):\n",
        "        if not isinstance(cur, dict) or k not in cur:\n",
        "            return (default, False) if _exist_flag else default\n",
        "        cur = cur[k]\n",
        "    return (cur, True) if _exist_flag else cur\n",
        "\n",
        "def iter_json_files(subdir: Path):\n",
        "    for p in subdir.glob(\"*.json\"):\n",
        "        yield p\n",
        "    for p in subdir.glob(\"*.JSON\"):\n",
        "        yield p\n",
        "\n",
        "def want_this_file(name: str) -> bool:\n",
        "    lower = name.lower()\n",
        "    if not lower.endswith(\".json\"):\n",
        "        return False\n",
        "    stem = lower[:-5]\n",
        "    parts = stem.rsplit(\"_\", 1)\n",
        "    if len(parts) != 2:\n",
        "        return False\n",
        "    suf = parts[1]\n",
        "    return suf.isdigit() and len(suf) == 2 and 0 <= int(suf) <= 8\n",
        "\n",
        "def process_one_label_group(label_group: str, label_root: Path) -> None:\n",
        "    group_dir = label_root / label_group\n",
        "    out_csv = group_dir / f\"{label_group}.csv\"\n",
        "    audit_csv = group_dir / f\"{label_group}_audit.csv\"\n",
        "\n",
        "    fieldnames = [\n",
        "        \"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\",\n",
        "        \"image_id\",\"gender\",\"age\",\"image_filename\",\n",
        "        \"skin_type\",\"sensitive\",\"forehead_pigmentation\",\"forehead_wrinkle\"\n",
        "    ]\n",
        "\n",
        "    audit_fields = [\n",
        "        \"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\",\n",
        "        \"reason\",\"detail_key\"\n",
        "    ]\n",
        "\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f, \\\n",
        "         open(audit_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as af:\n",
        "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        w.writeheader()\n",
        "        aw = csv.DictWriter(af, fieldnames=audit_fields)\n",
        "        aw.writeheader()\n",
        "\n",
        "        for subdir in sorted([p for p in group_dir.iterdir() if p.is_dir()]):\n",
        "            for jp in sorted([p for p in iter_json_files(subdir) if want_this_file(p.name)]):\n",
        "                suffix = jp.stem.rsplit(\"_\", 1)[-1]\n",
        "                row = {\n",
        "                    \"label_group\": label_group,\n",
        "                    \"subdir_name\": subdir.name,\n",
        "                    \"json_file\": jp.name,\n",
        "                    \"json_suffix\": suffix,\n",
        "                    \"image_id\": \"\", \"gender\": \"\", \"age\": \"\", \"image_filename\": \"\",\n",
        "                    \"skin_type\": \"\", \"sensitive\": \"\", \"forehead_pigmentation\": \"\", \"forehead_wrinkle\": \"\"\n",
        "                }\n",
        "\n",
        "                try:\n",
        "                    data = json.load(open(jp, \"r\", encoding=\"utf-8\"))\n",
        "                except Exception as e:\n",
        "                    aw.writerow({\n",
        "                        \"label_group\": label_group, \"subdir_name\": subdir.name,\n",
        "                        \"json_file\": jp.name, \"json_suffix\": suffix,\n",
        "                        \"reason\": \"json_read_error\", \"detail_key\": str(e)\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "                row[\"image_id\"]        = safe_get(data, \"info.id\", \"\")\n",
        "                row[\"gender\"]          = safe_get(data, \"info.gender\", \"\")\n",
        "                row[\"age\"]             = safe_get(data, \"info.age\", \"\")\n",
        "                row[\"image_filename\"]  = safe_get(data, \"info.filename\", \"\")\n",
        "\n",
        "                if suffix == \"00\":\n",
        "                    val, ok = safe_get(data, \"info.skin_type\", \"\", _exist_flag=True)\n",
        "                    row[\"skin_type\"] = \"\" if val is None else val\n",
        "                    if not ok:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"missing_key\",\"detail_key\":\"info.skin_type\"})\n",
        "                    elif val is None:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"null_value\",\"detail_key\":\"info.skin_type\"})\n",
        "\n",
        "                    val, ok = safe_get(data, \"info.sensitive\", \"\", _exist_flag=True)\n",
        "                    row[\"sensitive\"] = \"\" if val is None else val\n",
        "                    if not ok:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"missing_key\",\"detail_key\":\"info.sensitive\"})\n",
        "                    elif val is None:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"null_value\",\"detail_key\":\"info.sensitive\"})\n",
        "                else:\n",
        "                    val, ok = safe_get(data, \"annotations.forehead_pigmentation\", \"\", _exist_flag=True)\n",
        "                    row[\"forehead_pigmentation\"] = \"\" if val is None else val\n",
        "                    if not ok:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"missing_key\",\"detail_key\":\"annotations.forehead_pigmentation\"})\n",
        "                    elif val is None:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"null_value\",\"detail_key\":\"annotations.forehead_pigmentation\"})\n",
        "\n",
        "                    val, ok = safe_get(data, \"annotations.forehead_wrinkle\", \"\", _exist_flag=True)\n",
        "                    row[\"forehead_wrinkle\"] = \"\" if val is None else val\n",
        "                    if not ok:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"missing_key\",\"detail_key\":\"annotations.forehead_wrinkle\"})\n",
        "                    elif val is None:\n",
        "                        aw.writerow({**{k: row[k] for k in [\"label_group\",\"subdir_name\",\"json_file\",\"json_suffix\"]},\n",
        "                                     \"reason\":\"null_value\",\"detail_key\":\"annotations.forehead_wrinkle\"})\n",
        "\n",
        "                w.writerow(row)\n",
        "\n",
        "def main():\n",
        "    for g in LABEL_GROUPS:\n",
        "        process_one_label_group(g, LABEL_ROOT)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "E3xwHb1LdaWf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}